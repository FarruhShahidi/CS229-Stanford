\begin{answer}

i. Smaller learning rate make the convergence smaller, so it does not help\.\
ii. the difference between $\theta$ and the updated $\theta$ is learning rate time the gradient.
If we gradually decrease the learning rate, say to $1e-10,$ then sure, the algorithm stops.\\
iii. No, it will be still linear.
iv. This should help as the algorithm becomes non-linear.\\
v This should also help in the way as in the case of iv.
\end{answer}
