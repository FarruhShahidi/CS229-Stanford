\begin{answer}
    Using the Gaussian density, $g'(w^T_j x^{(i)}) = \frac{1}{2\sqrt \pi} \exp (-\frac{1}{2}(w^T_jx^{(i)})^2)$ rewrite the loss function
 $$\ell(W) = \sum_{i=1}^m\left(\log|W| + \sum_{j=1}^n (c - \frac {(w_j^Tx^{(i)})}{2})\right)
 $$  
 Using the gradient of the determinant(from the lecture notes), we have
    $$
\begin{aligned}
\nabla_Wl(W) &= \sum_{i=1}^m \left((W^{-1})^T + \sum_{j=1}^n\nabla_W(c - \frac{1}{2}(w_j^Tx^{(i)})^2\right)\\
&= \sum_{i=1}^m \left((W^{-1})^T  - Wx^{(i)}x^{(i)^T}\right)\\
&= m(W^{-1})^T - WX^TX
\end{aligned}
$$
By setting the loss function to zero, we get
$$
W^TW = \frac{1}{m}(X^TX)^{-1}.
$$

To see ambiguity mentioned in the lecture, we can see that if $W$ is a solution, then for an orthogonal matrix U, $UW$ is also a solution.
Indeed, if $U^TU = I$, then $(UW)^T(UW) = W^TU^TUW = W^TW = \frac{1}{m}(X^TX)^{-1}$

\end{answer}
